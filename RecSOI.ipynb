{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OA9A5tT61gc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle as pkl\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cY37P33Z66Sa"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "### Files needed (9 files):\n",
        "# subsampled_previous_work_abstracts.pkl: abtracts used to build the researcher profiles\n",
        "# co_authors_dict.pkl: dictionary containing the co-authors for the co-author heuristic\n",
        "# user_profiles_based_on_previous_work_abstracts_based_on_new_papers.pkl: mean of abstract embeddings used for testing vanilla sentence-BERT\n",
        "# subsampled_previous_work_abstracts_ner_clean.csv: previous work abstracts with keywords extracted using the NER tool of Raza et al. (2022)\n",
        "# new_subsampled_data.pkl: subsampled dataset of papers that contain SOIs\n",
        "# subsampled_data_ner_clean.pkl: NER concepts for the sentences in the subsampled dataset of papers\n",
        "# sentence_embeddings.pkl: embeddings of all sentences in the subsampled dataset of papers\n",
        "# LR_model.pkl: Logistic regression\n",
        "# list_of_weigthed_BERT_embeddings_profiles.pkl: embeddings of the researcher profiles\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52s9WbeX7fTe"
      },
      "outputs": [],
      "source": [
        "# When all the files are loaded on Colab (or are present locally), we can load them in the notebook.\n",
        "\n",
        "with open(\"subsampled_previous_work_abstracts.pkl\", \"rb\") as f:\n",
        "  subsampled_previous_work_abstracts = pkl.load(f)\n",
        "with open(\"co_authors_dict.pkl\", \"rb\") as f:\n",
        "  co_authors_dict = pkl.load(f)\n",
        "with open(\"user_profiles_based_on_previous_work_abstracts_based_on_new_papers.pkl\", \"rb\") as f:\n",
        "  user_profiles_based_on_previous_work_abstracts = pkl.load(f)\n",
        "with open(\"new_subsampled_data.pkl\", \"rb\") as f:\n",
        "  subsampled_data = pkl.load(f)\n",
        "with open(\"subsampled_data_ner_clean.pkl\", \"rb\") as f:\n",
        "  subsampled_data_NER = pkl.load(f)\n",
        "with open(\"subsampled_previous_work_abstracts_ner_clean.pkl\", \"rb\") as f:\n",
        "  abstracts_NER = pkl.load(f)\n",
        "with open(\"sentence_embeddings.pkl\", \"rb\") as f:\n",
        "  sentence_embeddings = pkl.load(f)\n",
        "with open(\"LR_model.pkl\", \"rb\") as f:\n",
        "  LR_model = pkl.load(f)\n",
        "with open(\"list_of_weigthed_BERT_embeddings_profiles.pkl\", \"rb\") as f:\n",
        "  weighted_BERT_embeddings_profiles = pkl.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlyPLrU58ETm"
      },
      "outputs": [],
      "source": [
        "# We add the concept extracted by the NER tool of Raza et al. (2022) in the DataFrame of abstracts\n",
        "\n",
        "abstracts_NER.index = subsampled_previous_work_abstracts.index\n",
        "subsampled_previous_work_abstracts[\"Concepts\"] = abstracts_NER[\"Word\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPosgeGhH00D"
      },
      "outputs": [],
      "source": [
        "# We add the concept extracted by the NER tool of Raza et al. (2022) in the DataFrame of papers containing the SOIs\n",
        "\n",
        "subsampled_data_NER.index = subsampled_data.index\n",
        "subsampled_data[\"Concepts\"] = subsampled_data_NER[\"Word\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AV1fayY87lw-"
      },
      "outputs": [],
      "source": [
        "# Check the number of unique authors used\n",
        "\n",
        "classes = list(subsampled_previous_work_abstracts[\"First Author\"])\n",
        "authors = np.unique(classes)\n",
        "print(f\"Number of authors = {len(authors)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxKGbKd19wzv"
      },
      "outputs": [],
      "source": [
        "# Prepare the TF-IDF vectorization of the abstracts\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "TFIDF_previous_work_abstracts = vectorizer.fit_transform(list(subsampled_previous_work_abstracts[\"Abstract\"]))\n",
        "previous_work_abstracts_column_names = vectorizer.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNrYIHcJ6_7u"
      },
      "outputs": [],
      "source": [
        "# Main part of the code to perform the experiment\n",
        "\n",
        "ranking = pd.DataFrame(columns=[\"Best Rank\", \"Number of Abstracts\"])\n",
        "list_of_sentence_BERT_ranks = []\n",
        "list_of_best_sentence_BERT_ranks = []\n",
        "\n",
        "list_of_LR_ranks = []\n",
        "list_of_best_LR_ranks = []\n",
        "\n",
        "list_of_RecSOI_ranks = []\n",
        "list_of_best_RecSOI_ranks = []\n",
        "\n",
        "number_of_abstracts = []\n",
        "df_of_best_ranked_SoIs_by_sentence_BERT = pd.DataFrame(index=subsampled_data.loc[subsampled_data['IGNORANCE_TYPE'] == 0].index) # We test for 0 because this is the label for SOIs in Boguslav et al.'s dataset\n",
        "df_of_best_ranked_SoIs_by_RecSOI = pd.DataFrame(index=subsampled_data.loc[subsampled_data['IGNORANCE_TYPE'] == 0].index) # We test for 0 because this is the label for SOIs in Boguslav et al.'s dataset\n",
        "\n",
        "all_results = pd.DataFrame(columns=[\"Expectancy Top5\", \"Expectancy Top10\", \"Expectancy Top20\", \"sentence-BERT Ranks\", \"LR Ranks\", \"RecSOI Ranks\"])\n",
        "all_co_author_results = pd.DataFrame(columns=[\"Expectancy Top5\", \"Expectancy Top10\", \"Expectancy Top20\", \"sentence-BERT Co-Author Ranks\", \"LR Co-Author Ranks\", \"RecSOI Co-Author Ranks\"])\n",
        "all_concept_results = pd.DataFrame(columns=[\"Expectancy Top5\", \"Expectancy Top10\", \"Expectancy Top20\", \"sentence-BERT Concept Ranks\", \"LR Concept Ranks\", \"RecSOI Concept Ranks\"])\n",
        "\n",
        "scaler = StandardScaler(with_mean=False).fit(TFIDF_previous_work_abstracts)\n",
        "baseline_results = []\n",
        "counter = 1\n",
        "for author in authors: # The recommendation is performed for each author\n",
        "  print(\"Working on\", author, \"(\", counter, \"on\", len(authors), \")\")\n",
        "\n",
        "  papers = subsampled_data[subsampled_data[\"First Author\"] == author]\n",
        "  SoIs_in_paper = subsampled_data[subsampled_data[\"IGNORANCE_TYPE\"] == 0] # 0 is used to say that it IS an ignorance in Boguslav et al. dataset and classifiers\n",
        "  investigated_SoI_indices = papers.loc[papers['IGNORANCE_TYPE'] == 0].index # 0 is used to say that it IS an ignorance in Boguslav et al. dataset and classifiers\n",
        "\n",
        "  co_author_papers = subsampled_data[subsampled_data[\"First Author\"].isin(co_authors_dict[author])]\n",
        "  investigated_co_auhor_SoI_indices = co_author_papers.loc[co_author_papers['IGNORANCE_TYPE'] == 0].index # 0 is used to say that it IS an ignorance in Boguslav et al. dataset and classifiers\n",
        "\n",
        "  # Gather the concepts of the authors based on what is extracted from their abstracts\n",
        "  author_concepts = []\n",
        "  author_abstracts = subsampled_previous_work_abstracts[subsampled_previous_work_abstracts[\"First Author\"] == author]\n",
        "  for _, abstract_info in author_abstracts.iterrows():\n",
        "    author_concepts += abstract_info[\"Concepts\"]\n",
        "  author_concepts = set(author_concepts)\n",
        "\n",
        "  # Gather the concept related to SOIs\n",
        "  investigated_concept_SoI_indices = []\n",
        "  for index, data_info in SoIs_in_paper.iterrows():\n",
        "    data_concepts = set(data_info[\"Concepts\"])\n",
        "    if len(data_concepts.intersection(author_concepts)) > 0:\n",
        "      investigated_concept_SoI_indices.append(index)\n",
        "  concept_papers = subsampled_data.loc[investigated_concept_SoI_indices]\n",
        "\n",
        "  if len(investigated_SoI_indices) == 0 and len(investigated_co_auhor_SoI_indices) == 0 and len(investigated_concept_SoI_indices) == 0:\n",
        "    counter += 1\n",
        "    continue\n",
        "\n",
        "  # Perform the recommendation for sentence-BERT\n",
        "  distances = []\n",
        "\n",
        "  statements_of_ignorance_indices = subsampled_data.loc[subsampled_data['IGNORANCE_TYPE'] == 0].index # 0 is used to say that it IS an ignorance\n",
        "  for statements_of_ignorance_index in statements_of_ignorance_indices:\n",
        "    distances.append(np.linalg.norm(user_profiles_based_on_previous_work_abstracts[author] - sentence_embeddings[statements_of_ignorance_index]))\n",
        "  distances = pd.DataFrame(distances)\n",
        "  distances.index = statements_of_ignorance_indices\n",
        "\n",
        "  # Perform the recommendation for LR\n",
        "  results = []\n",
        "\n",
        "  statements_of_ignorance_indices = subsampled_data.loc[subsampled_data['IGNORANCE_TYPE'] == 0].index # 0 is used to say that it IS an ignorance\n",
        "  TFIDF_statements_of_ignorance = vectorizer.transform(subsampled_data.loc[statements_of_ignorance_indices, \"SENTENCE\"])\n",
        "  scaled_TFIDF_statements_of_ignorance = scaler.transform(TFIDF_statements_of_ignorance)\n",
        "  results = np.transpose(LR_model.predict_proba(scaled_TFIDF_statements_of_ignorance))[list(LR_model.classes_).index(author)]\n",
        "  results = pd.DataFrame(results)\n",
        "  results.index = statements_of_ignorance_indices\n",
        "\n",
        "  # Perform the recommendation for RecSOI\n",
        "  RecSOI_results = []\n",
        "\n",
        "  statements_of_ignorance_indices = subsampled_data.loc[subsampled_data['IGNORANCE_TYPE'] == 0].index # 0 is used to say that it IS an ignorance\n",
        "  for statements_of_ignorance_index in statements_of_ignorance_indices:\n",
        "    RecSOI_temp_results = []\n",
        "    for author_embedding in weighted_BERT_embeddings_profiles[author]:\n",
        "      RecSOI_temp_results.append(np.linalg.norm(author_embedding - sentence_embeddings[statements_of_ignorance_index]))\n",
        "    RecSOI_results.append(min(RecSOI_temp_results))\n",
        "  RecSOI_results = pd.DataFrame(RecSOI_results)\n",
        "  RecSOI_results.index = statements_of_ignorance_indices\n",
        "\n",
        "  # Expectancy of results\n",
        "  N = len(statements_of_ignorance_indices)\n",
        "  K = len(investigated_SoI_indices)\n",
        "  all_results.loc[author, \"Expectancy Top5\"] = 0 if 5*(K/N) < 1 else 1\n",
        "  all_results.loc[author, \"Expectancy Top10\"] = 0 if 10*(K/N) < 1 else 1\n",
        "  all_results.loc[author, \"Expectancy Top20\"] = 0 if 20*(K/N) < 1 else 1\n",
        "\n",
        "  K = len(investigated_co_auhor_SoI_indices)\n",
        "  all_co_author_results.loc[author, \"Expectancy Top5\"] = 0 if 5*(K/N) < 1 else 1\n",
        "  all_co_author_results.loc[author, \"Expectancy Top10\"] = 0 if 10*(K/N) < 1 else 1\n",
        "  all_co_author_results.loc[author, \"Expectancy Top20\"] = 0 if 20*(K/N) < 1 else 1\n",
        "\n",
        "  K = len(investigated_concept_SoI_indices)\n",
        "  all_concept_results.loc[author, \"Expectancy Top5\"] = 0 if 5*(K/N) < 1 else 1\n",
        "  all_concept_results.loc[author, \"Expectancy Top10\"] = 0 if 10*(K/N) < 1 else 1\n",
        "  all_concept_results.loc[author, \"Expectancy Top20\"] = 0 if 20*(K/N) < 1 else 1\n",
        "\n",
        "  # Check that at least one SoI among the investigated ones has a good rank with sentence-BERT\n",
        "  ranks_of_SoI_of_interest = distances.rank().loc[investigated_SoI_indices, 0]\n",
        "  if len(ranks_of_SoI_of_interest) > 0:\n",
        "    all_results.loc[author, \"sentence-BERT Ranks\"] = min(list(ranks_of_SoI_of_interest))\n",
        "  else:\n",
        "    all_results.loc[author, \"sentence-BERT Ranks\"] = np.nan\n",
        "\n",
        "  df_of_best_ranked_SoIs_by_sentence_BERT.loc[statements_of_ignorance_indices, author] = distances.rank()[0]\n",
        "\n",
        "  # Check that at least one SoI among the ones from co-authors has a good rank with sentence-BERT\n",
        "  ranks_of_SoI_of_interest = distances.rank().loc[investigated_co_auhor_SoI_indices, 0]\n",
        "  if len(ranks_of_SoI_of_interest) > 0:\n",
        "    all_co_author_results.loc[author, \"sentence-BERT Co-Author Ranks\"] = min(list(ranks_of_SoI_of_interest))\n",
        "  else: # Means that there is no co-author of this author in the dataset\n",
        "    all_co_author_results.loc[author, \"sentence-BERT Co-Author Ranks\"] = np.nan\n",
        "\n",
        "  # Check that at least one SoI among the investigated ones has a concept in common with the author with sentence-BERT\n",
        "  ranks_of_SoI_of_interest = distances.rank().loc[investigated_concept_SoI_indices, 0]\n",
        "  if len(ranks_of_SoI_of_interest) > 0:\n",
        "    all_concept_results.loc[author, \"sentence-BERT Concept Ranks\"] = min(list(ranks_of_SoI_of_interest))\n",
        "  else: # Means that there is no SOI with matching concepts in the dataset\n",
        "    all_concept_results.loc[author, \"sentence-BERT Concept Ranks\"] = np.nan\n",
        "\n",
        "  # Check that at least one SoI among the investigated ones has a good rank with LR\n",
        "  ranks_of_SoI_of_interest = results.rank(ascending=False).loc[investigated_SoI_indices, 0]\n",
        "  if len(ranks_of_SoI_of_interest) > 0:\n",
        "    all_results.loc[author, \"LR Ranks\"] = min(list(ranks_of_SoI_of_interest))\n",
        "  else:\n",
        "    all_results.loc[author, \"LR Ranks\"] = np.nan\n",
        "\n",
        "  # Check that at least one SoI among the ones from co-authors has a good rank with LR\n",
        "  ranks_of_SoI_of_interest = results.rank(ascending=False).loc[investigated_co_auhor_SoI_indices, 0]\n",
        "  if len(ranks_of_SoI_of_interest) > 0:\n",
        "    all_co_author_results.loc[author, \"LR Co-Author Ranks\"] = min(list(ranks_of_SoI_of_interest))\n",
        "  else: # Means that there is no co-author of this author in the dataset\n",
        "    all_co_author_results.loc[author, \"LR Co-Author Ranks\"] = np.nan\n",
        "\n",
        "  # Check that at least one SoI among the investigated ones has a concept in common with the author with LR\n",
        "  ranks_of_SoI_of_interest = results.rank(ascending=False).loc[investigated_concept_SoI_indices, 0]\n",
        "  if len(ranks_of_SoI_of_interest) > 0:\n",
        "    all_concept_results.loc[author, \"LR Concept Ranks\"] = min(list(ranks_of_SoI_of_interest))\n",
        "  else: # Means that there is no SOI with matching concepts in the dataset\n",
        "    all_concept_results.loc[author, \"LR Concept Ranks\"] = np.nan\n",
        "\n",
        "  # Check that at least one SoI among the investigated ones has a good rank with RecSOI\n",
        "  ranks_of_SoI_of_interest = RecSOI_results.rank().loc[investigated_SoI_indices, 0]\n",
        "  if len(ranks_of_SoI_of_interest) > 0:\n",
        "    all_results.loc[author, \"RecSOI Ranks\"] = min(list(ranks_of_SoI_of_interest))\n",
        "  else:\n",
        "    all_results.loc[author, \"RecSOI Ranks\"] = np.nan\n",
        "\n",
        "  df_of_best_ranked_SoIs_by_RecSOI.loc[statements_of_ignorance_indices, author] = RecSOI_results.rank()[0]\n",
        "\n",
        "  # Check that at least one SoI among the ones from co-authors has a good rank with RecSOI\n",
        "  ranks_of_SoI_of_interest = RecSOI_results.rank().loc[investigated_co_auhor_SoI_indices, 0]\n",
        "  if len(ranks_of_SoI_of_interest) > 0:\n",
        "    all_co_author_results.loc[author, \"RecSOI Co-Author Ranks\"] = min(list(ranks_of_SoI_of_interest))\n",
        "  else: # Means that there is no co-author of this author in the dataset\n",
        "    all_co_author_results.loc[author, \"RecSOI Co-Author Ranks\"] = np.nan\n",
        "\n",
        "  # Check that at least one SoI among the investigated ones has a concept in common with the author with RecSOI\n",
        "  ranks_of_SoI_of_interest = RecSOI_results.rank().loc[investigated_concept_SoI_indices, 0]\n",
        "  if len(ranks_of_SoI_of_interest) > 0:\n",
        "    all_concept_results.loc[author, \"RecSOI Concept Ranks\"] = min(list(ranks_of_SoI_of_interest))\n",
        "  else: # Means that there is no SOI with matching concepts in the dataset\n",
        "    all_concept_results.loc[author, \"RecSOI Concept Ranks\"] = np.nan\n",
        "\n",
        "  counter += 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the total number of abstracts. We need this info because we want to be flexible in the number of maximum abstracts we allow during the evaluation.\n",
        "\n",
        "for author in all_results.index:\n",
        "  all_results.loc[author, \"Number of Abstracts\"] = len(subsampled_previous_work_abstracts[subsampled_previous_work_abstracts[\"First Author\"] == author])\n",
        "for author in all_co_author_results.index:\n",
        "  all_co_author_results.loc[author, \"Number of Abstracts\"] = len(subsampled_previous_work_abstracts[subsampled_previous_work_abstracts[\"First Author\"] == author])\n",
        "for author in all_concept_results.index:\n",
        "  all_concept_results.loc[author, \"Number of Abstracts\"] = len(subsampled_previous_work_abstracts[subsampled_previous_work_abstracts[\"First Author\"] == author])"
      ],
      "metadata": {
        "id": "b__rQVe8EcHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0Mf_HtgQXTX"
      },
      "outputs": [],
      "source": [
        "def author_heuristic(top=5, max_number_abstracts=5):\n",
        "  \"\"\"\n",
        "  Compute the author heuristic: how good we are at recommending SOIs of author A with author A's profile?\n",
        "  Then show the results.\n",
        "  \"\"\"\n",
        "  print(f\"Results for the author heuristic, a number of abstracts = {max_number_abstracts}, and MAP@{top}\")\n",
        "  temp_all_results = all_results[all_results[\"Number of Abstracts\"] <= max_number_abstracts]\n",
        "\n",
        "  num_best_sentence_BERT_ranks = len(temp_all_results[temp_all_results[\"sentence-BERT Ranks\"] <= top])\n",
        "  num_best_LR_ranks = len(temp_all_results[temp_all_results[\"LR Ranks\"] <= top])\n",
        "  num_best_RecSOI_ranks = len(temp_all_results[temp_all_results[\"RecSOI Ranks\"] <= top])\n",
        "\n",
        "  print(\"Total number of authors:\", len(temp_all_results))\n",
        "  print(\"Percentage of authors for which expectancy of the number of random successes >= 1:\", np.mean(temp_all_results[f\"Expectancy Top{top}\"], axis=0))\n",
        "  print(f\"Number of authors with at least one SoI ranked <= {top} for sentence-BERT:\", num_best_sentence_BERT_ranks, \"(\"+str(num_best_sentence_BERT_ranks/len(temp_all_results))+\")\")\n",
        "  print(f\"Number of authors with at least one SoI ranked <= {top} for LR:\", num_best_LR_ranks, \"(\"+str(num_best_LR_ranks/len(temp_all_results))+\")\")\n",
        "  print(f\"Number of authors with at least one SoI ranked <= {top} for RecSOI:\", num_best_RecSOI_ranks, \"(\"+str(num_best_RecSOI_ranks/len(temp_all_results))+\")\")\n",
        "  print(\"-------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8ZBwbC-_AS9",
        "outputId": "9e8ad552-96d5-436b-9fcc-0836a955d798"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for the author heuristic, a number of abstracts = 5, and MAP@5\n",
            "Total number of authors: 500\n",
            "Percentage of authors for which expectancy of the number of random successes >= 1: 0.0\n",
            "Number of authors with at least one SoI ranked <= 5 for sentence-BERT: 156 (0.312)\n",
            "Number of authors with at least one SoI ranked <= 5 for LR: 115 (0.23)\n",
            "Number of authors with at least one SoI ranked <= 5 for RecSOI: 173 (0.346)\n",
            "-------------------\n",
            "Results for the author heuristic, a number of abstracts = 5, and MAP@10\n",
            "Total number of authors: 500\n",
            "Percentage of authors for which expectancy of the number of random successes >= 1: 0.0\n",
            "Number of authors with at least one SoI ranked <= 10 for sentence-BERT: 189 (0.378)\n",
            "Number of authors with at least one SoI ranked <= 10 for LR: 141 (0.282)\n",
            "Number of authors with at least one SoI ranked <= 10 for RecSOI: 210 (0.42)\n",
            "-------------------\n",
            "Results for the author heuristic, a number of abstracts = 5, and MAP@20\n",
            "Total number of authors: 500\n",
            "Percentage of authors for which expectancy of the number of random successes >= 1: 0.0\n",
            "Number of authors with at least one SoI ranked <= 20 for sentence-BERT: 228 (0.456)\n",
            "Number of authors with at least one SoI ranked <= 20 for LR: 191 (0.382)\n",
            "Number of authors with at least one SoI ranked <= 20 for RecSOI: 234 (0.468)\n",
            "-------------------\n"
          ]
        }
      ],
      "source": [
        "author_heuristic(5)\n",
        "author_heuristic(10)\n",
        "author_heuristic(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YLTtpi6Qnn5"
      },
      "outputs": [],
      "source": [
        "def co_author_heuristic(top=5, max_number_abstracts=5):\n",
        "  \"\"\"\n",
        "  Compute the co-author heuristic: how good we are at recommending SOIs of the co-authors of author A with author A's profile?\n",
        "  Then show the results.\n",
        "  \"\"\"\n",
        "  print(f\"Results for the co-author heuristic, a number of abstracts = {max_number_abstracts}, and MAP@{top}\")\n",
        "  mask = list((~all_co_author_results.isna())[\"sentence-BERT Co-Author Ranks\"]) # We only want to consider authors that have a co-author as first-author in the dataset\n",
        "  temp_all_co_author_results = all_co_author_results[mask]\n",
        "  temp_all_co_author_results = temp_all_co_author_results[temp_all_co_author_results[\"Number of Abstracts\"] <= max_number_abstracts]\n",
        "\n",
        "  num_best_sentence_BERT_co_author_ranks = len(temp_all_co_author_results[temp_all_co_author_results[\"sentence-BERT Co-Author Ranks\"] <= top])\n",
        "  num_best_LR_co_author_ranks = len(temp_all_co_author_results[temp_all_co_author_results[\"LR Co-Author Ranks\"] <= top])\n",
        "  num_best_RecSOI_co_author_ranks = len(temp_all_co_author_results[temp_all_co_author_results[\"RecSOI Co-Author Ranks\"] <= top])\n",
        "\n",
        "  print(\"Total number of authors:\", len(temp_all_co_author_results))\n",
        "  print(\"Percentage of authors for which expectancy of the number of random successes >= 1:\", np.mean(temp_all_co_author_results[f\"Expectancy Top{top}\"], axis=0))\n",
        "  print(f\"Number of authors with at least one SoI ranked <= {top} for sentence-BERT:\", num_best_sentence_BERT_co_author_ranks, \"(\"+str(num_best_sentence_BERT_co_author_ranks/len(temp_all_co_author_results))+\")\")\n",
        "  print(f\"Number of authors with at least one SoI ranked <= {top} for LR:\", num_best_LR_co_author_ranks, \"(\"+str(num_best_LR_co_author_ranks/len(temp_all_co_author_results))+\")\")\n",
        "  print(f\"Number of authors with at least one SoI ranked <= {top} for RecSOI:\", num_best_RecSOI_co_author_ranks, \"(\"+str(num_best_RecSOI_co_author_ranks/len(temp_all_co_author_results))+\")\")\n",
        "  print(\"-------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XenaOOte_UCk",
        "outputId": "5982d591-5bb7-4f85-929e-41c5573b9f45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for the co-author heuristic, a number of abstracts = 5, and MAP@5\n",
            "Total number of authors: 59\n",
            "Percentage of authors for which expectancy of the number of random successes >= 1: 0.0\n",
            "Number of authors with at least one SoI ranked <= 5 for sentence-BERT: 8 (0.13559322033898305)\n",
            "Number of authors with at least one SoI ranked <= 5 for LR: 4 (0.06779661016949153)\n",
            "Number of authors with at least one SoI ranked <= 5 for RecSOI: 6 (0.1016949152542373)\n",
            "-------------------\n",
            "Results for the co-author heuristic, a number of abstracts = 5, and MAP@10\n",
            "Total number of authors: 59\n",
            "Percentage of authors for which expectancy of the number of random successes >= 1: 0.0\n",
            "Number of authors with at least one SoI ranked <= 10 for sentence-BERT: 9 (0.15254237288135594)\n",
            "Number of authors with at least one SoI ranked <= 10 for LR: 5 (0.0847457627118644)\n",
            "Number of authors with at least one SoI ranked <= 10 for RecSOI: 11 (0.1864406779661017)\n",
            "-------------------\n",
            "Results for the co-author heuristic, a number of abstracts = 5, and MAP@20\n",
            "Total number of authors: 59\n",
            "Percentage of authors for which expectancy of the number of random successes >= 1: 0.0\n",
            "Number of authors with at least one SoI ranked <= 20 for sentence-BERT: 14 (0.23728813559322035)\n",
            "Number of authors with at least one SoI ranked <= 20 for LR: 7 (0.11864406779661017)\n",
            "Number of authors with at least one SoI ranked <= 20 for RecSOI: 14 (0.23728813559322035)\n",
            "-------------------\n"
          ]
        }
      ],
      "source": [
        "co_author_heuristic(5)\n",
        "co_author_heuristic(10)\n",
        "co_author_heuristic(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8vSBCgxQ4_-"
      },
      "outputs": [],
      "source": [
        "def concept_heuristic(top=5, max_number_abstracts=5):\n",
        "  \"\"\"\n",
        "  Compute the concept heuristic: how good we are at recommending SOIs of with relevant concepts for each author?\n",
        "  Then show the results.\n",
        "  \"\"\"\n",
        "  print(f\"Results for the concept heuristic, a number of abstracts = {max_number_abstracts}, and MAP@{top}\")\n",
        "  mask = list((~all_concept_results.isna())[\"sentence-BERT Concept Ranks\"]) # We only want to consider authors that have matching concepts with at least one SOI in the dataset\n",
        "  tmp_all_concept_results = all_concept_results[mask]\n",
        "  tmp_all_concept_results = tmp_all_concept_results[tmp_all_concept_results[\"Number of Abstracts\"] <= max_number_abstracts]\n",
        "\n",
        "  num_best_sentence_BERT_concept_ranks = len(tmp_all_concept_results[tmp_all_concept_results[\"sentence-BERT Concept Ranks\"] <= top])\n",
        "  num_best_LR_concept_ranks = len(tmp_all_concept_results[tmp_all_concept_results[\"LR Concept Ranks\"] <= top])\n",
        "  num_best_RecSOI_concept_ranks = len(tmp_all_concept_results[tmp_all_concept_results[\"RecSOI Concept Ranks\"] <= top])\n",
        "\n",
        "  print(\"Total number of authors:\", len(tmp_all_concept_results))\n",
        "  print(\"Percentage of authors for which expectancy of the number of random successes >= 1:\", np.mean(tmp_all_concept_results[f\"Expectancy Top{top}\"], axis=0))\n",
        "  print(f\"Number of authors with at least one SoI ranked <= {top} for sentence-BERT:\", num_best_sentence_BERT_concept_ranks, \"(\"+str(num_best_sentence_BERT_concept_ranks/len(tmp_all_concept_results))+\")\")\n",
        "  print(f\"Number of authors with at least one SoI ranked <= {top} for LR:\", num_best_LR_concept_ranks, \"(\"+str(num_best_LR_concept_ranks/len(tmp_all_concept_results))+\")\")\n",
        "  print(f\"Number of authors with at least one SoI ranked <= {top} for RecSOI:\", num_best_RecSOI_concept_ranks, \"(\"+str(num_best_RecSOI_concept_ranks/len(tmp_all_concept_results))+\")\")\n",
        "  print(\"-------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcUg5JQRQVD1",
        "outputId": "1bd72f86-ee74-4404-9c4c-21220d01d882"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for the concept heuristic, a number of abstracts = 5, and MAP@5\n",
            "Total number of authors: 496\n",
            "Percentage of authors for which expectancy of the number of random successes >= 1: 0.0\n",
            "Number of authors with at least one SoI ranked <= 5 for sentence-BERT: 324 (0.6532258064516129)\n",
            "Number of authors with at least one SoI ranked <= 5 for LR: 215 (0.4334677419354839)\n",
            "Number of authors with at least one SoI ranked <= 5 for RecSOI: 375 (0.7560483870967742)\n",
            "-------------------\n",
            "Results for the concept heuristic, a number of abstracts = 5, and MAP@10\n",
            "Total number of authors: 496\n",
            "Percentage of authors for which expectancy of the number of random successes >= 1: 0.1592741935483871\n",
            "Number of authors with at least one SoI ranked <= 10 for sentence-BERT: 369 (0.7439516129032258)\n",
            "Number of authors with at least one SoI ranked <= 10 for LR: 288 (0.5806451612903226)\n",
            "Number of authors with at least one SoI ranked <= 10 for RecSOI: 418 (0.842741935483871)\n",
            "-------------------\n",
            "Results for the concept heuristic, a number of abstracts = 5, and MAP@20\n",
            "Total number of authors: 496\n",
            "Percentage of authors for which expectancy of the number of random successes >= 1: 0.4596774193548387\n",
            "Number of authors with at least one SoI ranked <= 20 for sentence-BERT: 408 (0.8225806451612904)\n",
            "Number of authors with at least one SoI ranked <= 20 for LR: 370 (0.7459677419354839)\n",
            "Number of authors with at least one SoI ranked <= 20 for RecSOI: 451 (0.9092741935483871)\n",
            "-------------------\n"
          ]
        }
      ],
      "source": [
        "concept_heuristic(5)\n",
        "concept_heuristic(10)\n",
        "concept_heuristic(20)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}